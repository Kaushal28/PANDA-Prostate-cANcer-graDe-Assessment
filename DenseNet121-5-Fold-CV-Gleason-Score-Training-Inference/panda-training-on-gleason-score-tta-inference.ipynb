{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing /kaggle/input/pretrainedmodelswhl/pretrainedmodels-0.7.4-py3-none-any.whl\r\n",
      "Requirement already satisfied: torchvision in /opt/conda/lib/python3.7/site-packages (from pretrainedmodels==0.7.4) (0.5.0)\r\n",
      "Requirement already satisfied: tqdm in /opt/conda/lib/python3.7/site-packages (from pretrainedmodels==0.7.4) (4.43.0)\r\n",
      "Requirement already satisfied: munch in /opt/conda/lib/python3.7/site-packages (from pretrainedmodels==0.7.4) (2.5.0)\r\n",
      "Requirement already satisfied: torch in /opt/conda/lib/python3.7/site-packages (from pretrainedmodels==0.7.4) (1.4.0)\r\n",
      "Requirement already satisfied: numpy in /opt/conda/lib/python3.7/site-packages (from torchvision->pretrainedmodels==0.7.4) (1.18.1)\r\n",
      "Requirement already satisfied: six in /opt/conda/lib/python3.7/site-packages (from torchvision->pretrainedmodels==0.7.4) (1.14.0)\r\n",
      "Requirement already satisfied: pillow>=4.1.1 in /opt/conda/lib/python3.7/site-packages (from torchvision->pretrainedmodels==0.7.4) (5.4.1)\r\n",
      "Installing collected packages: pretrainedmodels\r\n",
      "Successfully installed pretrainedmodels-0.7.4\r\n",
      "Processing /kaggle/input/testtimeaug/ttach-0.0.2-py3-none-any.whl\r\n",
      "Installing collected packages: ttach\r\n",
      "Successfully installed ttach-0.0.2\r\n"
     ]
    }
   ],
   "source": [
    "!pip install /kaggle/input/pretrainedmodelswhl/pretrainedmodels-0.7.4-py3-none-any.whl\n",
    "!pip install /kaggle/input/testtimeaug/ttach-0.0.2-py3-none-any.whl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import math\n",
    "import openslide\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "import albumentations\n",
    "from tqdm import tqdm\n",
    "from joblib import Parallel, delayed\n",
    "from matplotlib import pyplot as plt\n",
    "from PIL import Image, ImageChops\n",
    "import pretrainedmodels\n",
    "\n",
    "import cv2\n",
    "\n",
    "import torch\n",
    "import torch.utils.data as data_utils\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms\n",
    "from torch.optim import lr_scheduler\n",
    "from torch import nn\n",
    "\n",
    "from torchvision import transforms,models\n",
    "import torch.nn.functional as F\n",
    "from tqdm.auto import tqdm\n",
    "from torch import Tensor\n",
    "\n",
    "import ttach as tta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "BASE_DIR = '/kaggle/input/prostate-cancer-grade-assessment'\n",
    "# DATA_DIR = os.path.join(BASE_DIR, 'train_images')\n",
    "DATA_DIR = os.path.join(BASE_DIR, 'test_images')\n",
    "DEVICE = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_df = pd.read_csv(os.path.join(BASE_DIR, 'train.csv'))[['image_id', 'data_provider']].loc[:10]\n",
    "# test_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df = pd.read_csv(os.path.join(BASE_DIR, 'test.csv'))\n",
    "sample_sub_df = pd.read_csv(os.path.join(BASE_DIR, 'sample_submission.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>image_id</th>\n",
       "      <th>data_provider</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>005700be7e06878e6605e7a5a39de1b2</td>\n",
       "      <td>radboud</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>005c6e8877caf724c600fdce5d417d40</td>\n",
       "      <td>karolinska</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0104f76634ff89bfff1ef0804a95c380</td>\n",
       "      <td>radboud</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                           image_id data_provider\n",
       "0  005700be7e06878e6605e7a5a39de1b2       radboud\n",
       "1  005c6e8877caf724c600fdce5d417d40    karolinska\n",
       "2  0104f76634ff89bfff1ef0804a95c380       radboud"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "org_test_df = test_df.copy()\n",
    "org_test_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "crop_size = 256  # Size of resultant images\n",
    "crop_level = 2  # The level of slide used to get the images (you can use 0 to get very high resolution images)\n",
    "down_samples = [1, 4, 16]  # List of down samples available in any tiff image file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_image(openslide_image):\n",
    "    \"\"\"\n",
    "    Splits the given image into multiple images if 256x256\n",
    "    \"\"\"\n",
    "    \n",
    "    # Get the size of the given image\n",
    "    width, height = openslide_image.level_dimensions[crop_level]\n",
    "\n",
    "    # Get the dimensions of level 0 resolution, as it's required in \"read_region()\" function\n",
    "    base_height = down_samples[crop_level] * height  # height of level 0\n",
    "    base_width = down_samples[crop_level] * width  # width of level 0\n",
    "\n",
    "    # Get the number of smaller images \n",
    "    h_crops = math.ceil(width / crop_size)\n",
    "    v_crops = math.ceil(height / crop_size)\n",
    "\n",
    "    splits = []\n",
    "    for v in range(v_crops):\n",
    "        for h in range(h_crops): \n",
    "            x_location = h*crop_size*down_samples[crop_level]\n",
    "            y_location = v*crop_size*down_samples[crop_level]\n",
    "\n",
    "            patch = openslide_image.read_region((x_location, y_location), crop_level, (crop_size, crop_size))\n",
    "\n",
    "            splits.append(patch)\n",
    "    return splits, h_crops, v_crops"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_emptiness(arr):\n",
    "    total_ele = arr.size\n",
    "    white_ele = np.count_nonzero(arr == 255) + np.count_nonzero(arr == 0)\n",
    "    return white_ele / total_ele"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "ignore_threshold = 0.95  # If the image is more than 95% empty, consider it as white and ignore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_white_images(images):\n",
    "    non_empty_crops = []\n",
    "    for image in images:\n",
    "        image_arr = np.array(image)[...,:3]  # Discard the alpha channel\n",
    "        emptiness = get_emptiness(image_arr)\n",
    "        if emptiness < ignore_threshold:\n",
    "            non_empty_crops.append(image)\n",
    "    return non_empty_crops"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = []\n",
    "def create_dataset(count):\n",
    "    img = os.path.join(DATA_DIR, f'{test_df[\"image_id\"].iloc[count]}.tiff')\n",
    "    img = openslide.OpenSlide(img)\n",
    "    crops, _, _ = split_image(img)\n",
    "    img.close()\n",
    "\n",
    "    non_empty_crops = filter_white_images(crops)\n",
    "    image_id = test_df['image_id'].iloc[count]\n",
    "\n",
    "    for index, img in enumerate(non_empty_crops):\n",
    "        img_metadata = {}\n",
    "        img = img.convert('RGB')\n",
    "\n",
    "        img_metadata['image_id'] = f'{image_id}_{index}'\n",
    "        img_metadata['data_provider'] = test_df['data_provider'].iloc[count]\n",
    "        img_metadata['group'] = count\n",
    "\n",
    "        img.save(f'{image_id}_{index}.jpg', 'JPEG', quality=100, optimize=True, progressive=True)\n",
    "        dataset.append(img_metadata)\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "if os.path.exists(DATA_DIR):\n",
    "    dataset = Parallel(n_jobs=8)(delayed(create_dataset)(count) for count in tqdm(range(len(test_df))))\n",
    "    dataset = [item for sublist in dataset for item in sublist]\n",
    "\n",
    "    dataset = pd.DataFrame(dataset)\n",
    "    dataset.to_csv('new_test.csv', index=False)\n",
    "    test_df = pd.read_csv('new_test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a"
   },
   "outputs": [],
   "source": [
    "class DenseNet121(nn.Module):\n",
    "    \"\"\"\n",
    "    Define DenseNet121 model with 10 output classes based on gleason scores\n",
    "    \"\"\"\n",
    "    def __init__(self, pretrained):\n",
    "        super(DenseNet121, self).__init__()\n",
    "        if pretrained is True:\n",
    "            self.model = pretrainedmodels.__dict__[\"densenet121\"](pretrained=\"imagenet\")\n",
    "        else:\n",
    "            self.model = pretrainedmodels.__dict__[\"densenet121\"](pretrained=None)\n",
    "        \n",
    "        self.l0 = nn.Linear(1024, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        bs, _, _, _ = x.shape\n",
    "        x = self.model.features(x)\n",
    "        x = F.adaptive_avg_pool2d(x, 1).reshape(bs, -1)\n",
    "        l0 = self.l0(x)\n",
    "        return l0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "FOLDS = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define TTA parameters\n",
    "transforms = tta.Compose(\n",
    "    [\n",
    "        tta.HorizontalFlip(),\n",
    "        tta.VerticalFlip(),\n",
    "        tta.Scale(scales=[1, 2])       \n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = []\n",
    "for fold in range(FOLDS):\n",
    "    model = DenseNet121(False)\n",
    "    model.to(DEVICE)\n",
    "    model.load_state_dict(torch.load(f'/kaggle/input/panda-densenet121-labelsmoothing-tta-fold{fold}/densenet121_ls_fold{fold}.pth', map_location=DEVICE))\n",
    "    model = tta.ClassificationTTAWrapper(model, transforms)\n",
    "    model.eval()\n",
    "    models.append(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "WORKING_DIR = os.path.join('/', 'kaggle', 'working')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PandaDataset(Dataset):\n",
    "    \"\"\"Custom dataset for PANDA Tests\"\"\"\n",
    "    \n",
    "    def __init__(self, df, mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225)):\n",
    "        self.df = df\n",
    "        self.aug = albumentations.Compose([\n",
    "            albumentations.Normalize(mean, std, always_apply=True)\n",
    "        ])\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        image_id = self.df.loc[index]['image_id']\n",
    "        image = cv2.imread(os.path.join(WORKING_DIR, f'{image_id}.jpg'))\n",
    "        image = self.aug(image=image)['image']\n",
    "        \n",
    "        # Convert from NHWC to NCHW as pytorch expects images in NCHW format\n",
    "        image = np.transpose(image, (2, 0, 1))\n",
    "        \n",
    "        # For now, just return image and ISUP grades\n",
    "        return image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE=16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def inference(model, test_loader, device):\n",
    "    preds = []\n",
    "    for i, images in tqdm(enumerate(test_loader)):\n",
    "        images = images.to(device, dtype=torch.float)/255\n",
    "            \n",
    "        with torch.no_grad():\n",
    "            y_preds = model(images)\n",
    "        preds.append(y_preds.to('cpu').numpy())\n",
    "    preds = np.concatenate(preds)\n",
    "    return preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gleason score 6 = ISUP grade 1\n",
    "# Gleason score 7 (3 + 4) = ISUP grade 2\n",
    "# Gleason score 7 (4 + 3) = ISUP grade 3\n",
    "# Gleason score 8 = ISUP grade 4\n",
    "# Gleason score 9-10 = ISUP grade 5\n",
    "\n",
    "'''\n",
    "Mappings of gleason scores when model was trained\n",
    "     0: '0+0',\n",
    "     1: '3+3',\n",
    "     2: '3+4',\n",
    "     3: '3+5',\n",
    "     4: '4+3',\n",
    "     5: '4+4',\n",
    "     6: '4+5',\n",
    "     7: '5+3',\n",
    "     8: '5+4',\n",
    "     9: '5+5'\n",
    "'''\n",
    "\n",
    "mappings = {\n",
    "    0: 0, # (0+0: ISUP grade 0)\n",
    "    1: 1, # (3+3: ISUP grade 1)\n",
    "    2: 2, # (3+4: ISUP grade 2)\n",
    "    3: 4, # (3+5: ISUP grade 4)\n",
    "    4: 3, # (4+3: ISUP grade 3)\n",
    "    5: 4, # (4+4: ISIP grade 4)\n",
    "    6: 5, # (4+5: ISUP grade 5)\n",
    "    7: 4, # (5+3: ISUP grade 4)\n",
    "    8: 5, # (5+4: ISUP grade 5)\n",
    "    9: 5  # (5+5: ISUP grade 5)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def submit(sample):\n",
    "    global sample_sub_df\n",
    "    if os.path.exists(DATA_DIR):\n",
    "        test_dataset = PandaDataset(test_df)\n",
    "        test_loader = data_utils.DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
    "        \n",
    "        preds = []\n",
    "        for fold in range(FOLDS):\n",
    "            preds.append(inference(models[fold], test_loader, DEVICE))\n",
    "        preds = np.array(preds)\n",
    "        preds = np.argmax(np.mean(preds, axis=0), axis=1)\n",
    "        preds = np.vectorize(mappings.get)(preds)\n",
    "        test_df['preds'] = preds\n",
    "        sample = sample.drop(['data_provider'], axis=1)\n",
    "        sample['isup_grade'] = test_df.groupby('group')['preds'].agg(lambda x:x.value_counts().index[0])\n",
    "        return sample\n",
    "    return sample_sub_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>image_id</th>\n",
       "      <th>isup_grade</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>005700be7e06878e6605e7a5a39de1b2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>005c6e8877caf724c600fdce5d417d40</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0104f76634ff89bfff1ef0804a95c380</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                           image_id  isup_grade\n",
       "0  005700be7e06878e6605e7a5a39de1b2           0\n",
       "1  005c6e8877caf724c600fdce5d417d40           0\n",
       "2  0104f76634ff89bfff1ef0804a95c380           0"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "submission = submit(org_test_df)\n",
    "submission['isup_grade'] = submission['isup_grade'].astype(int)\n",
    "submission.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "!rm -rf *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission.to_csv('submission.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
