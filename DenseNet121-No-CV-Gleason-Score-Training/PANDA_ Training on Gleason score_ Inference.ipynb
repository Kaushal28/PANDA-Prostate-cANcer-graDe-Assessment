{"cells":[{"metadata":{"trusted":true},"cell_type":"code","source":"!pip install /kaggle/input/pretrainedmodelswhl/pretrainedmodels-0.7.4-py3-none-any.whl","execution_count":9,"outputs":[{"output_type":"stream","text":"Requirement already satisfied: pretrainedmodels==0.7.4 from file:///kaggle/input/pretrainedmodelswhl/pretrainedmodels-0.7.4-py3-none-any.whl in /opt/conda/lib/python3.7/site-packages (0.7.4)\nRequirement already satisfied: torchvision in /opt/conda/lib/python3.7/site-packages (from pretrainedmodels==0.7.4) (0.5.0)\nRequirement already satisfied: torch in /opt/conda/lib/python3.7/site-packages (from pretrainedmodels==0.7.4) (1.4.0)\nRequirement already satisfied: munch in /opt/conda/lib/python3.7/site-packages (from pretrainedmodels==0.7.4) (2.5.0)\nRequirement already satisfied: tqdm in /opt/conda/lib/python3.7/site-packages (from pretrainedmodels==0.7.4) (4.43.0)\nRequirement already satisfied: numpy in /opt/conda/lib/python3.7/site-packages (from torchvision->pretrainedmodels==0.7.4) (1.18.1)\nRequirement already satisfied: six in /opt/conda/lib/python3.7/site-packages (from torchvision->pretrainedmodels==0.7.4) (1.14.0)\nRequirement already satisfied: pillow>=4.1.1 in /opt/conda/lib/python3.7/site-packages (from torchvision->pretrainedmodels==0.7.4) (5.4.1)\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"import os\n\nimport math\nimport openslide\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\nimport albumentations\nfrom tqdm import tqdm\nfrom joblib import Parallel, delayed\nfrom matplotlib import pyplot as plt\nfrom PIL import Image, ImageChops\nimport pretrainedmodels\n\nimport cv2\n\nimport torch\nimport torch.utils.data as data_utils\nfrom torch.utils.data import Dataset, DataLoader\nfrom torchvision import transforms\nfrom torch.optim import lr_scheduler\nfrom torch import nn\n\nfrom torchvision import transforms,models\nimport torch.nn.functional as F\nfrom tqdm.auto import tqdm\nfrom torch import Tensor","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"BASE_DIR = '/kaggle/input/prostate-cancer-grade-assessment'\n# DATA_DIR = os.path.join(BASE_DIR, 'train_images')\nDATA_DIR = os.path.join(BASE_DIR, 'test_images')\nDEVICE = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# test_df = pd.read_csv(os.path.join(BASE_DIR, 'train.csv'))[['image_id', 'data_provider']].loc[:10]\n# test_df","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_df = pd.read_csv(os.path.join(BASE_DIR, 'test.csv'))\nsample_sub_df = pd.read_csv(os.path.join(BASE_DIR, 'sample_submission.csv'))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"org_test_df = test_df.copy()\norg_test_df","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"crop_size = 256  # Size of resultant images\ncrop_level = 2  # The level of slide used to get the images (you can use 0 to get very high resolution images)\ndown_samples = [1, 4, 16]  # List of down samples available in any tiff image file","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def split_image(openslide_image):\n    \"\"\"\n    Splits the given image into multiple images if 256x256\n    \"\"\"\n    \n    # Get the size of the given image\n    width, height = openslide_image.level_dimensions[crop_level]\n\n    # Get the dimensions of level 0 resolution, as it's required in \"read_region()\" function\n    base_height = down_samples[crop_level] * height  # height of level 0\n    base_width = down_samples[crop_level] * width  # width of level 0\n\n    # Get the number of smaller images \n    h_crops = math.ceil(width / crop_size)\n    v_crops = math.ceil(height / crop_size)\n\n    splits = []\n    for v in range(v_crops):\n        for h in range(h_crops): \n            x_location = h*crop_size*down_samples[crop_level]\n            y_location = v*crop_size*down_samples[crop_level]\n\n            patch = openslide_image.read_region((x_location, y_location), crop_level, (crop_size, crop_size))\n\n            splits.append(patch)\n    return splits, h_crops, v_crops","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def get_emptiness(arr):\n    total_ele = arr.size\n    white_ele = np.count_nonzero(arr == 255) + np.count_nonzero(arr == 0)\n    return white_ele / total_ele","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"ignore_threshold = 0.95  # If the image is more than 95% empty, consider it as white and ignore","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def filter_white_images(images):\n    non_empty_crops = []\n    for image in images:\n        image_arr = np.array(image)[...,:3]  # Discard the alpha channel\n        emptiness = get_emptiness(image_arr)\n        if emptiness < ignore_threshold:\n            non_empty_crops.append(image)\n    return non_empty_crops","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"dataset = []\ndef create_dataset(count):\n    img = os.path.join(DATA_DIR, f'{test_df[\"image_id\"].iloc[count]}.tiff')\n    img = openslide.OpenSlide(img)\n    crops, _, _ = split_image(img)\n    img.close()\n\n    non_empty_crops = filter_white_images(crops)\n    image_id = test_df['image_id'].iloc[count]\n\n    for index, img in enumerate(non_empty_crops):\n        img_metadata = {}\n        img = img.convert('RGB')\n\n        img_metadata['image_id'] = f'{image_id}_{index}'\n        img_metadata['data_provider'] = test_df['data_provider'].iloc[count]\n        img_metadata['group'] = count\n\n        img.save(f'{image_id}_{index}.jpg', 'JPEG', quality=100, optimize=True, progressive=True)\n        dataset.append(img_metadata)\n    return dataset","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"if os.path.exists(DATA_DIR):\n    dataset = Parallel(n_jobs=8)(delayed(create_dataset)(count) for count in tqdm(range(len(test_df))))\n    dataset = [item for sublist in dataset for item in sublist]\n\n    dataset = pd.DataFrame(dataset)\n    dataset.to_csv('new_test.csv', index=False)\n    test_df = pd.read_csv('new_test.csv')","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"class DenseNet121(nn.Module):\n    \"\"\"\n    Define DenseNet121 model with 10 output classes based on gleason scores\n    \"\"\"\n    def __init__(self, pretrained):\n        super(DenseNet121, self).__init__()\n        if pretrained is True:\n            self.model = pretrainedmodels.__dict__[\"densenet121\"](pretrained=\"imagenet\")\n        else:\n            self.model = pretrainedmodels.__dict__[\"densenet121\"](pretrained=None)\n        \n        self.l0 = nn.Linear(1024, 10)\n\n    def forward(self, x):\n        bs, _, _, _ = x.shape\n        x = self.model.features(x)\n        x = F.adaptive_avg_pool2d(x, 1).reshape(bs, -1)\n        l0 = self.l0(x)\n        return l0","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"paths = ['panda-training-with-pre-trained-densenet121']\nFOLDS = 1","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"models = []\nfor fold in range(FOLDS):\n    model = DenseNet121(False)\n    model.to(DEVICE)\n#     model = nn.DataParallel(model)\n    model.load_state_dict(torch.load(f'/kaggle/input/{paths[fold]}/densenet121_256_gs_fold{fold}.pth', map_location=DEVICE))\n    model.eval()\n    models.append(model)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"WORKING_DIR = os.path.join('/', 'kaggle', 'working')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"class PandaDataset(Dataset):\n    \"\"\"Custom dataset for PANDA Tests\"\"\"\n    \n    def __init__(self, df, mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225)):\n        self.df = df\n        self.aug = albumentations.Compose([\n            albumentations.Normalize(mean, std, always_apply=True)\n        ])\n        \n    def __len__(self):\n        return len(self.df)\n    \n    def __getitem__(self, index):\n        image_id = self.df.loc[index]['image_id']\n        image = cv2.imread(os.path.join(WORKING_DIR, f'{image_id}.jpg'))\n        image = self.aug(image=image)['image']\n        \n        # Convert from NHWC to NCHW as pytorch expects images in NCHW format\n        image = np.transpose(image, (2, 0, 1))\n        \n        # For now, just return image and ISUP grades\n        return image","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"BATCH_SIZE=16","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def inference(model, test_loader, device):\n    preds = []\n    for i, images in tqdm(enumerate(test_loader)):\n        images = images.to(device, dtype=torch.float)/255\n            \n        with torch.no_grad():\n            y_preds = model(images)\n        preds.append(y_preds.to('cpu').numpy())\n    preds = np.concatenate(preds)\n    return preds","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Gleason score 6 = ISUP grade 1\n# Gleason score 7 (3 + 4) = ISUP grade 2\n# Gleason score 7 (4 + 3) = ISUP grade 3\n# Gleason score 8 = ISUP grade 4\n# Gleason score 9-10 = ISUP grade 5\n\n'''\nMappings of gleason scores when model was trained\n     0: '0+0',\n     1: '3+3',\n     2: '3+4',\n     3: '3+5',\n     4: '4+3',\n     5: '4+4',\n     6: '4+5',\n     7: '5+3',\n     8: '5+4',\n     9: '5+5'\n'''\n\nmappings = {\n    0: 0, # (0+0: ISUP grade 0)\n    1: 1, # (3+3: ISUP grade 1)\n    2: 2, # (3+4: ISUP grade 2)\n    3: 4, # (3+5: ISUP grade 4)\n    4: 3, # (4+3: ISUP grade 3)\n    5: 4, # (4+4: ISIP grade 4)\n    6: 5, # (4+5: ISUP grade 5)\n    7: 4, # (5+3: ISUP grade 4)\n    8: 5, # (5+4: ISUP grade 5)\n    9: 5  # (5+5: ISUP grade 5)\n}","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def submit(sample):\n    global sample_sub_df\n    if os.path.exists(DATA_DIR):\n        test_dataset = PandaDataset(test_df)\n        test_loader = data_utils.DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False)\n        \n        preds = []\n        for fold in range(FOLDS):\n            preds.append(inference(models[fold], test_loader, DEVICE))\n        preds = np.array(preds)\n        preds = np.argmax(np.mean(preds, axis=0), axis=1)\n        preds = np.vectorize(mappings.get)(preds)\n        test_df['preds'] = preds\n        sample = sample.drop(['data_provider'], axis=1)\n        sample['isup_grade'] = test_df.groupby('group')['preds'].agg(lambda x:x.value_counts().index[0])\n        return sample\n    return sample_sub_df","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"submission = submit(org_test_df)\nsubmission['isup_grade'] = submission['isup_grade'].astype(int)\nsubmission.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"!rm -rf *","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"submission.to_csv('submission.csv', index=False)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}